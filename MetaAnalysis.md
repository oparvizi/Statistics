Source: Oxford Handbook of Medical Statistics: https://academic.oup.com/book/24583

## Meta- analysis         

   Hierarchies of evidence                         
    <img width="267" alt="image" src="https://user-images.githubusercontent.com/105786517/220347405-269f0d21-6ee3-48e2-9151-362a946fab71.png">       

   Combining multiple studies
   Criticisms of evidence hierarchies     
   Systematic reviews: ummarize all scientific evidence relating to a particular research question include the followingsources:
        
        • Computerized databases such as PubMed and MEDLINE          
        • Bibliographies of textbooks        
        • References in published original studies and in review articles       
        • Registers of studies conducted       
        • Personal communication with specialists in the field of interest         
        It is usually necessary to search multiple sources for several reasons:        
        • E lectronic databases may not be totally complete due to the accidental omission of some publications        
        • Studies may only be listed in a specialist database, such as the AMED database for studies in allied and complementary medicine          
        • Some newer medical journals may not yet be indexed in PubMed          
            
   Search strategy: needs to be tailored to the purpose of the study   
   
            • Identify abstracts in the subject area       
            • Read and discard those that are inappropriate        
            • Obtain full versions of all potentially appropriate publications and discard those that are then shown to be inappropriate Choosing search terms for                   electronic searching         
            • Use a combination of recognized words— MeSH (medical subjectheading)           
            • Watch out for UK versus US spellings            
            • Check that the search strategy has worked                    
        Extracting the relevant data        
        Reporting results: PRISMA         
  Meta-analysis: combine the results from multiple studies to produce a single estimate.
     why?
     
        • To pool all findings on a topic to gain an overall view           
        • To increase statistical power compared with individual studies           
        • To improve estimates of effect size       
        • To resolve controversies when the findings of studies disagree         
        • To answer new questions not addressed in individual studies        
   Protocol for meta- analysis:
     
        • Aims of the meta- analysis
        • Rules for inclusion and exclusion of studies
        • Search strategies
        • Statistical methods
   Good meta- analysis?
   
            • The meta- analysis has a clear question
            • All relevant evidence has been gathered
            • The individual study estimates have been evaluated to ensure that studies are sufficiently similar to be pooled
            • Publication bias has been considered and addressed as appropriate
            • The data have been suitably analysed and presented with a clear description of how the meta- analysis was conducted in accordance
            with the PRISMA guidelines
   Sample size for meta- analysis:
    
            • The number of studies in a meta- analysis obviously varies according to what research has been previously conducted in a specific area
            • The greater the number of studies, the greater the precision of the pooled estimate in a meta- analysis
            • The most important issue is that the studies represent the totality of evidence to provide an unbiased overall estimate
            • It may be perfectly reasonable to pool just three or four studies if they are all that exist
        
                                        A large meta- analysis that obtains only a subset of all studies because
                                            of publication bias may give a very precise estimate but it may be 
                                                        ------------- biased.-------------
       
   Combining estimates in meta- analyses:
   
         - Vote counting: numbers of studies showing statistically significant (‘positive’) results are counted.
         - Sign test: is reasonable where no numerical data are provided from studies but the direction of the effect is known, or where studies are so diverse that a                        pooled estimate makes no sense
         - Combining P values
         - Weighting effect estimates
         - Software: RevMan, CMA, Stata, SPSS, SAS, and R
   Combining different effect measures
   
         - Cohen’s d  
                d = difference in means/standard deviation   ,   odds ratios (ORs)
         - Cautions: Sensitivity analyses   
<img width="328" alt="image" src="https://user-images.githubusercontent.com/105786517/220352739-ff1a3ccb-67bd-4e1d-bff8-4b57a68f9ea1.png">   
         
   Heterogeneity: is usually taken to mean that there is observed variability between study estimates.
   
       Tests for heterogeneity
                • In general, the test is conservative and so a non- significant result cannot be interpreted as showing that there is no heterogeneity. 
                  For this reason, a cut- off of P <0.10 is commonly used rather than P <0.05 to indicate heterogeneity
                • T he test itself does not provide an estimate of the degree of heterogeneity (see the I2 statistic in the following section)
                • Like all statistical tests, this test is less powerful when the number of studies is small (the sample size), and is very powerful when 
                  the number of studies is large
                • T he test is a statistical tool and does not on its own provide any insight into the reasons for any heterogeneity that exists
       The I^2 statistic
            a descriptive statistic that provides an estimate of the proportion of the total variability between estimates that can be attributed to heterogeneity
            itself. In other words, it indicates what proportion of the observed variability reflects real differences in effect size and so ranges from 0 to 100%. It             is based on the test statistic Q, calculated to test for heterogeneity. Hence I2 is larger when there is more
            heterogeneity.
       Sources of heterogeneity: 
               Possible clinical sources of heterogeneity include:
                • T reatment differences in RCTs (e.g. doses, other medications given)
                • V ariation in patients (e.g. age, sex, diagnosis, etc.)
                • V ariation in study design (e.g. parallel group versus crossover design for trials, cohort versus case– control for observational studies)    
<img width="335" alt="image" src="https://user-images.githubusercontent.com/105786517/220353498-0797c2b3-a1ce-48d3-a964-fe4e5ed8a03b.png">          
        
       Addressing heterogeneity
          - Fixed and random effects
          - Meta- analysis for heterogeneous studies
          - Meta- regression
Fixed effects estimates       
      
      Formulae for meta- analysis
 <img width="331" alt="image" src="https://user-images.githubusercontent.com/105786517/220354929-67a4a1df-7d8e-4c39-a2b7-cc2fded5c233.png">      
      
      Calculating a fixed effects pooled odds ratio            
 <img width="326" alt="image" src="https://user-images.githubusercontent.com/105786517/220355088-e87a3eea-30ab-407e-9d94-5c11da1b9c1d.png">        
     
      Using a statistical program
Random effects estimates
 
      - Within and between study variability
      - Random effects weights
      - Using a statistical program
Formulae for meta- analyses        
 <img width="323" alt="image" src="https://user-images.githubusercontent.com/105786517/220355568-1812f1f6-2a17-4764-b5be-21548b9590a8.png">         
 
Presenting meta- analyses        
 
    Forest plots    
 <img width="325" alt="image" src="https://user-images.githubusercontent.com/105786517/220355840-fe43cd11-9b59-45bc-8333-6cadead30d87.png"><img width="329" alt="image" src="https://user-images.githubusercontent.com/105786517/220355956-4b296f9e-a7f0-4f5d-8247-3eef33d5b32c.png"><img width="326" alt="image" src="https://user-images.githubusercontent.com/105786517/220356044-6fc2162e-fec1-49c8-a41e-8406a4e88cac.png">                

Publication bias:           
  
    - Publication bias occurs when the articles that are published on a topic are an incomplete subset of all the studies that have been conducted on that topic.      
    
    Several reasons why publication bias happens: Statistical significance, Fashion and popularity, Sponsorship, Language.
    
    There is much evidence to show that studies which have statistically significant results are more likely to be published that those which have not. This
    can happen because:
        • T he author either does not write up the work and submit an article at all, or after submitting an article and getting a rejection, gives up
        • T he journal editors reject articles reporting non- significant findings because they are thought to be uninteresting and/ or non- informative
        • Researchers conduct exploratory analyses on many outcomes and only the significant ones are written up
 
    - Consequences of publication bias: leads to inflated estimates whereby the overall size of effect is exaggerated, Delayed publication         
    - Reducing publication bias: Registration of study protocols, Publication of negative studies         
 
Detecting publication bias           
  
    - Funnel plots
      A funnel plot is a simple graphical method for exploring the results from studies to see if publication bias might be present. It works as follows:
        • The magnitude of study effect is plotted against a measure of study precision, such as the inverse of the variance or standard error, or the sample size
        • As the precision (sample size) increases, the range of estimates becomes narrower, showing a funnel shape
        • If there is no publication bias the plot will be symmetrical about the pooled value for all the studies, because small imprecise studies with
          negative results are as likely to be published as small studies with positive results
        • If, however, more small studies with positive findings reach publication than small studies with negative findings, the wide section of the funnel
          will not be symmetrical— there will be ‘holes’ in the plot
 Is there publication bias or not?               
 
        • It will be obvious if there is substantial asymmetry but it may be harder to differentiate between slight asymmetry and random variation
        • T here are statistical tests, such as Begg’s rank correlation test and the linear regression test by Egger, which are described by Sutton and a simulated               example is shown in Figure 13.6. These tests can be applied to aid decision- making but have limitations in how they perform in different situations and               should at best be regarded as a guide              
  <img width="326" alt="image" src="https://user-images.githubusercontent.com/105786517/220358675-8c527478-f22c-42c4-816f-8a704b304b3e.png">          
  Adjusting for publication bias
  
      Trim and fill
        This method works in the following way:
        • A funnel plot is drawn
        • Small studies are removed until the plot is symmetrical
        • T he true centre of the plot is estimated
        • T he ‘trimmed’ studies are replaced with their reflections
        • T he effect size is re- estimated and the number of ‘missing’ studies is noted           
  <img width="329" alt="image" src="https://user-images.githubusercontent.com/105786517/220359498-1fcba5c0-1bf5-452b-ab2f-109b938e5f9d.png">                   
       
       Regression method           
  <img width="323" alt="image" src="https://user-images.githubusercontent.com/105786517/220359898-e9a942f6-539f-484c-89f8-8fd7177693cd.png">          
      
      Which method to use?
        • Moreno and colleagues’ article (2009) compared the performance of trim and fill and several versions of the regression method and
        concluded that regression- based adjustments for publication bias were more reliable than trim and fill methods
        • A more recent publication has reviewed over 50 methods and concluded that it is difficult to say which is best as they all have
        limitations and their validity is rarely tested (Mueller et al. 2016)
        • In the absence of clear evidence- based guidelines, it would seem prudent to use and interpret methods for adjusting for publication bias
        with caution        
  Independent patient data meta- analysis           
     
     Limitations of study- level meta- analyses              
     Independent patient data (IPD) meta- analysis         
  
  Challenges in meta- analysis     
  
    Trial designs                
    Observational study designs       
    Disparate outcomes
    Number needed to treat (NNT)                 
  <img width="331" alt="image" src="https://user-images.githubusercontent.com/105786517/220361215-f2e64e4e-9c46-4527-8c17-c2406ac2da10.png">           
  

